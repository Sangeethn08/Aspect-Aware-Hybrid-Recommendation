{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54d00cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: (507418, 12)\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# Load CSV and preprocess\n",
    "# ========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load CSV\n",
    "data = pd.read_csv(\"finaldataset.csv\")\n",
    "\n",
    "# Ensure proper types\n",
    "data[\"timestamp\"] = pd.to_datetime(\n",
    "    data[\"timestamp\"],      \n",
    "    format=\"%d-%m-%Y %H:%M:%S\",  \n",
    "    errors=\"coerce\"\n",
    ")\n",
    "data[\"overview\"] = data[\"overview\"].astype(str)\n",
    "data[\"genres\"] = data[\"genres\"].astype(str)\n",
    "data[\"keywords\"] = data[\"keywords\"].astype(str)\n",
    "data[\"title\"] = data[\"title\"].astype(str)\n",
    "\n",
    "print(\"Data loaded:\", data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6984a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: (492295, 12)\n",
      "Test data: (15120, 12)\n"
     ]
    }
   ],
   "source": [
    "# Train/test split per user\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for user in data[\"userId\"].unique():\n",
    "    user_df = data[data[\"userId\"] == user].sort_values(\"timestamp\")\n",
    "    if len(user_df) <= 3:\n",
    "        continue\n",
    "    train_data.append(user_df.iloc[:-3])\n",
    "    test_data.append(user_df.iloc[-3:])\n",
    "\n",
    "train_data = pd.concat(train_data)\n",
    "test_data = pd.concat(test_data)\n",
    "\n",
    "print(\"Train data:\", train_data.shape)\n",
    "print(\"Test data:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb17f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Extract aspects from overview, keywords, genres\n",
    "# ========================\n",
    "import re\n",
    "\n",
    "def extract_aspects(row):\n",
    "    # Combine textual features\n",
    "    text = row[\"overview\"] + \" \" + row[\"keywords\"] + \" \" + row[\"genres\"]\n",
    "    # Extract meaningful word pairs\n",
    "    return \" \".join(set(re.findall(r\"\\b\\w+\\s\\w+\\b\", text.lower())))\n",
    "\n",
    "train_data[\"aspect_text\"] = train_data.apply(extract_aspects, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie aspect matrix shape: (3822, 1000)\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# Convert aspects to TF-IDF vectors\n",
    "# ========================\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "movie_text = train_data.drop_duplicates(\"movieId\")[[\"movieId\", \"aspect_text\"]]\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "movie_vectors = tfidf.fit_transform(movie_text[\"aspect_text\"])\n",
    "\n",
    "movie_vector_df = pd.DataFrame(\n",
    "    movie_vectors.toarray(),\n",
    "    index=movie_text[\"movieId\"],\n",
    "    columns=tfidf.get_feature_names_out()\n",
    ")\n",
    "\n",
    "print(\"Movie aspect matrix shape:\", movie_vector_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74085b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Trainable Tiny Neural Network for Aspect Scores\n",
    "# ========================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TinyAspectNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 1)  # predict user preference score\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "# Initialize model\n",
    "input_dim = movie_vector_df.shape[1]\n",
    "tiny_nn = TinyAspectNN(input_dim)\n",
    "\n",
    "# Loss + optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(tiny_nn.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a9541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset for NN\n",
    "train_X = []\n",
    "train_y = []\n",
    "\n",
    "for _, row in train_data.iterrows():\n",
    "    movie_vec = movie_vector_df.loc[row[\"movieId\"]].values\n",
    "    train_X.append(movie_vec)\n",
    "    train_y.append(row[\"rating\"])\n",
    "\n",
    "train_X = torch.tensor(np.array(train_X), dtype=torch.float32)\n",
    "train_y = torch.tensor(np.array(train_y), dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fca28ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 13.6786\n",
      "Epoch 2/5, Loss: 12.8778\n",
      "Epoch 3/5, Loss: 12.0039\n",
      "Epoch 4/5, Loss: 11.0193\n",
      "Epoch 5/5, Loss: 9.9505\n"
     ]
    }
   ],
   "source": [
    "# Tiny training loop (fast, CPU-friendly)\n",
    "tiny_nn.train()\n",
    "epochs = 5  # very small number to keep it fast\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = tiny_nn(train_X)\n",
    "    loss = criterion(outputs, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adee4001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== \n",
    "# Build user aspect vector\n",
    "# ========================\n",
    "def build_user_aspect_vector(user_id, alpha=0.7):\n",
    "    user_movies = train_data[train_data[\"userId\"] == user_id].sort_values(\"timestamp\")\n",
    "    user_vector = np.zeros(movie_vector_df.shape[1])\n",
    "    for _, row in user_movies.iterrows():\n",
    "        movie_vec = movie_vector_df.loc[row[\"movieId\"]].values\n",
    "        user_vector = alpha * user_vector + (1 - alpha) * movie_vec\n",
    "    return user_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41d80f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Compute aspect-based similarity scores\n",
    "# ========================\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def aspect_score(user_id):\n",
    "    tiny_nn.eval()\n",
    "    with torch.no_grad():\n",
    "        scores = tiny_nn(torch.tensor(movie_vector_df.values, dtype=torch.float32)).numpy()\n",
    "    return pd.Series(scores, index=movie_vector_df.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Build user-item matrix & SVD\n",
    "# ========================\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "user_item = train_data.pivot_table(\n",
    "    index=\"userId\",\n",
    "    columns=\"movieId\",\n",
    "    values=\"rating\"\n",
    ").fillna(0)\n",
    "\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "user_factors = svd.fit_transform(user_item)\n",
    "item_factors = svd.components_\n",
    "\n",
    "def cf_predict(user_id, movie_id):\n",
    "    if user_id not in user_item.index or movie_id not in user_item.columns:\n",
    "        return 0\n",
    "    u = user_item.index.get_loc(user_id)\n",
    "    m = user_item.columns.get_loc(movie_id)\n",
    "    return np.dot(user_factors[u], item_factors[:, m])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3da67f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Hybrid recommendation: CF + Aspect\n",
    "# ========================\n",
    "def hybrid_recommend(user_id, top_n=5, w_cf=0.4, w_aspect=0.6):\n",
    "    if user_id not in train_data[\"userId\"].unique():\n",
    "        top_movies = train_data.groupby(\"movieId\").size().sort_values(ascending=False).head(top_n)\n",
    "        return pd.Series(1.0, index=top_movies.index)\n",
    "    \n",
    "    aspect_scores = aspect_score(user_id)\n",
    "    cf_scores = pd.Series({movie: cf_predict(user_id, movie) for movie in movie_vector_df.index})\n",
    "    \n",
    "    final_scores = w_aspect * aspect_scores + w_cf * cf_scores\n",
    "    \n",
    "    watched = set(train_data[train_data[\"userId\"] == user_id][\"movieId\"])\n",
    "    final_scores = final_scores.drop(index=watched, errors=\"ignore\")\n",
    "    \n",
    "    return final_scores.sort_values(ascending=False).head(top_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65f9216a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tiny NN weights saved\n"
     ]
    }
   ],
   "source": [
    "# Save only model weights\n",
    "torch.save(tiny_nn.state_dict(), \"tiny_aspect_nn_weights.pth\")\n",
    "print(\"✅ Tiny NN weights saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cebf8933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tiny NN model loaded with weights\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model first (same architecture)\n",
    "tiny_nn_loaded = TinyAspectNN(input_dim=movie_vector_df.shape[1])\n",
    "\n",
    "# Load weights\n",
    "tiny_nn_loaded.load_state_dict(torch.load(\"tiny_aspect_nn_weights.pth\"))\n",
    "tiny_nn_loaded.eval()  # set to evaluation mode\n",
    "print(\"✅ Tiny NN model loaded with weights\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17d6035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saisa\\AppData\\Local\\Temp\\ipykernel_7416\\3634121594.py:12: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.set_index(\"movieId\")[\"rating\"].to_dict())\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# Prepare test movies & ratings dictionaries\n",
    "# ========================\n",
    "test_movies = (\n",
    "    test_data.groupby(\"userId\")[\"movieId\"]\n",
    "    .apply(set)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "test_ratings = (\n",
    "    test_data.groupby(\"userId\")\n",
    "    .apply(lambda x: x.set_index(\"movieId\")[\"rating\"].to_dict())\n",
    "    .to_dict()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@5: 0.017063492063492062\n",
      "Recall@5: 0.028439153439153434\n",
      "NDCG@5: 0.02354420462661541\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# Evaluation using NN-based hybrid\n",
    "# ========================\n",
    "def precision_recall_at_k_nn(k=5):\n",
    "    precisions, recalls = [], []\n",
    "\n",
    "    for user, true_movies in test_movies.items():\n",
    "        recs = hybrid_recommend(user, top_n=k).index  # compute dynamically\n",
    "        hits = len(set(recs) & true_movies)\n",
    "        precisions.append(hits / k)\n",
    "        recalls.append(hits / len(true_movies))\n",
    "\n",
    "    return np.mean(precisions), np.mean(recalls)\n",
    "\n",
    "\n",
    "def ndcg_at_k_nn(k=5):\n",
    "    ndcgs = []\n",
    "\n",
    "    for user, rating_dict in test_ratings.items():\n",
    "        recs = hybrid_recommend(user, top_n=k).index  # compute dynamically\n",
    "        dcg = 0.0\n",
    "        for i, movie in enumerate(recs):\n",
    "            if movie in rating_dict:\n",
    "                dcg += (2 ** rating_dict[movie] - 1) / np.log2(i + 2)\n",
    "\n",
    "        ideal = sorted(rating_dict.values(), reverse=True)[:k]\n",
    "        idcg = sum((2 ** r - 1) / np.log2(i + 2) for i, r in enumerate(ideal))\n",
    "\n",
    "        if idcg > 0:\n",
    "            ndcgs.append(dcg / idcg)\n",
    "\n",
    "    return np.mean(ndcgs)\n",
    "\n",
    "\n",
    "p, r = precision_recall_at_k_nn(k=5)\n",
    "print(\"Precision@5:\", p)\n",
    "print(\"Recall@5:\", r)\n",
    "print(\"NDCG@5:\", ndcg_at_k_nn(k=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "080347cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 Recommended Movies for User 10:\n",
      "movieId\n",
      "1291    Indiana Jones and the Last Crusade\n",
      "1193       One Flew Over the Cuckoo's Nest\n",
      "1213                            GoodFellas\n",
      "110                             Braveheart\n",
      "1270                    Back to the Future\n",
      "Name: title, dtype: object\n",
      "\n",
      "Hybrid Scores:\n",
      " movieId\n",
      "1291    1.160771\n",
      "1193    1.136716\n",
      "1213    1.073246\n",
      "110     1.052303\n",
      "1270    1.046009\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# Show top-5 recommendations for a user (input-based)\n",
    "# ========================\n",
    "user_input = input(\"Enter User ID to get recommendations: \")\n",
    "\n",
    "# Convert to int safely\n",
    "try:\n",
    "    user_id = int(user_input)\n",
    "except ValueError:\n",
    "    print(\"⚠️ Invalid input. Using default User ID = 15\")\n",
    "    user_id = 15\n",
    "\n",
    "# Check if user exists in train_data\n",
    "if user_id not in train_data[\"userId\"].unique():\n",
    "    print(f\"User ID {user_id} not found. Showing top-rated movies instead.\")\n",
    "    top_movies = train_data.groupby(\"movieId\")[\"rating\"].mean().sort_values(ascending=False).head(5)\n",
    "    titles = data.drop_duplicates(\"movieId\").set_index(\"movieId\")[\"title\"]\n",
    "    print(\"Top-5 Rated Movies:\")\n",
    "    print(titles.loc[top_movies.index])\n",
    "else:\n",
    "    # Get hybrid recommendations\n",
    "    recs = hybrid_recommend(user_id)\n",
    "    titles = data.drop_duplicates(\"movieId\").set_index(\"movieId\")[\"title\"]\n",
    "    print(f\"Top-5 Recommended Movies for User {user_id}:\")\n",
    "    print(titles.loc[recs.index])\n",
    "    print(\"\\nHybrid Scores:\\n\", recs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
